% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%


\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% subfigures, side by side
\usepackage{subcaption}

% hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
% \usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% Allows umlaut and non ascii characters
\usepackage[utf8]{inputenc}

% Insert blank pages
\usepackage{afterpage}
%\afterpage{\null\newpage}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

% Turn off bibliography header
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

% No page numbers
\pagenumbering{gobble}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%\begin{center}
%    \large Summer 2017 Research Application
%
%    \normalsize Clark Fitzgerald
%\end{center}
%
%\emph{Indicate the dates when you will be here at UC Davis during the summer of
%2017.}
%
%I will be at UC Davis the entire summer.
%   
%\emph{Indicate whether you have other summer employment (TA / GSR  / Reader / Internship).
%If so, please indicate dates:}
%
%I will be a TA for STA 13 for the first summer session, June 26 - August 4.
%I will also be on active NSF fellowship status.
%   
%\emph{Please list one faculty member who can endorse your
%research activities (pertaining to point 1) during the summer of 2017.}
%
%Professor Duncan Temple Lang
%
%\newpage
%
%\begin{center}
%    \large Automatic Parallelism Through R Code Analysis 
%\end{center}
%
%\hfill
%
%In summer of 2017 I plan to develop experimental software capable of
%converting serial R programs into parallel programs using multiprocessing.
%This automatic program tranformation differs from current parallel
%technologies which require the user to explicitly write code for a given
%parallel programming model. The software will have two major components:
%performance profiling, and programmatic transformation of the code into a
%parallel version.
%
%% This is useful because it allows R programs to scale up and use the
%% computational resources available on modern machines. 
%% Since the
%% program conversion happens automatically this allows us to build more
%% intelligence into the system, freeing the user to think about the specifics
%% of the problem and write higher level code, knowing that the system will do
%% the best it can to find a way to parallelize the operation.
%
%Opportunities for parallelism can be found through analysis of base R's
%apply family of functions using the CodeDepends package
%\cite{R-CodeDepends}. The apply family includes \texttt{lapply, apply, sapply,
%tapply, by, mapply, Map, vapply, outer, by, replicate}. These are all
%variants of the map reduce computational model which has
%been successful for implementing large scale parallel systems
%%\cite{dean2008mapreduce}. 
%
%Performance profiling measures how long the program spends executing each
%part of the code. The profiling together with estimates of the overhead
%associated with multiprocessing on the particular machine allow us to
%determine if it's actually worth it to parallelize a given R expression.
%If the parallel version is slower then the expression should
%be left in serial form.
%
%I will apply the software to several test cases including:
%\begin{enumerate}
%    \item Basic algorithms operating on $n \times p$ matrices, such as
%        covariance calculations and nearest neighbors
%    \item Statistical simulations using \texttt{replicate()}
%    \item Practical data analysis on 100's of GB of traffic sensor data 
%\end{enumerate}
%To pass the tests the software should be capable of generating a parallel
%program that rivals the speed of a hand written parallel version. The
%practical data analysis use case will be the most challenging, because it's
%more difficult to profile, and the physical organization of data on disk
%imposes additional constraints.
%Other general challenges include respecting R's dynamic scoping rules,
%avoiding nested parallelism, and handling complex control flow.
%
%By the end of the summer I expect to produce a working prototype of the
%system that can handle basic algorithms and simulations. The broader goal
%is to incorporate more intelligence into the system, freeing the user to
%write higher level code that also performs better.
%
%% As an example, the below code calculates column medians for a random matrix
%% of size $n \times p$. Fix $n$ and we can empirically show that
%% parallelism with 2 cores based on process forking becomes more efficient
%% than the serial version for $p < P$.
%% \begin{verbatim}
%% x = matrix(rnorm(n * p), nrow = n)
%% 
%% # Calculating the column medians is 
%% column_medians = apply(x, 2, median)
%% \end{verbatim}
%% 
%
%\newpage
%

\begin{center}
    \large Automatic Parallelism Through R Code Analysis 

    \normalsize 2017 UC Davis Statistics Summer Report
    
    Clark Fitzgerald
\end{center}

\hfill

This summer I began development on the \texttt{autoparallel} package for R
to automatically convert serial code to parallel. It's publicly available
at \url{https://github.com/clarkfitzg/autoparallel}. The \texttt{CodeDepends}
package~\cite{R-CodeDepends}
provides underlying tools for static code analysis.
This work creates new possibilities for automatic parallelization in R in
three different ways: interactive function development, code benchmarking,
and function metaprogramming.

\texttt{autoparallel} supports iterative, interactive function development
on large data sets. It does this by automatically distributing data across
a cluster exactly once, and then returning a closure which evaluates serial
code on each cluster node. The package inspects the code to determine which
variables (including functions) the code uses. It then exports the
discovered variables to the workers so they always have the most current
versions.  A user may define a data analysis function, quickly test on a
large data set, redefine the function and repeat. R users are accustomed to
this workflow. \texttt{autoparallel} makes this same workflow possible on a
parallel cluster.

Automatic parallelization of code happens through substitution of
(\texttt{lapply, apply, mapply, Map}) with their parallel equivalents.
\texttt{autoparallel} runs the code, benchmarking wall time for serial and
parallel versions of each statment.  The version with the faster median
time is chosen and included in the transformed program.  A disadvantage of
this approach is that it requires many evaluations of expensive functions,
so it only applies if the code will be used many times in a similar manner.

The most exciting development is in the function metaprogramming. A direct
way to use this is to provide multiple implementations for a function along
with statistical models for how long they will take. For example, let $X$
be an $n \times p$ matrix of real numbers. Calling \texttt{crossprod(X)} in
R will compute the matrix $X^T X$. This requires $O(np^2)$ operations.
A linear model works well to predict the run
time:
\[
    t = \beta_0 + \beta_1 np^2 + \epsilon
\]
Every time
\texttt{crossprod()} is called we collect another timing data point which
can be used to update $\hat{\beta}_0, \hat{\beta}_1$. \texttt{autoparallel}
defines methods such as \texttt{predict()} for a class of functions that dynamically
choose the fastest implementations based on models trained on all
previously seen inputs.

%ie.  equation~\ref{eq:f}
%above. I'm currently working to generalize this function metaprogramming to
%include the ability to modify and record the operating characterisitcs of a
%wider class of functions, including those in base R.

\texttt{autoparallel} provides parallelism with interactive features, the
ability to programmatically transform code, and functions which can tune
themselves to their inputs.
The documentation
shows examples of how to use the tools for covariance calculations, linear
discriminant analysis, and analysis of 1.3 GB of text data. I plan to
continue work on this package and release in on CRAN by the end of the year
once the user facing interfaces stabilize.

\hfill

\bibliographystyle{plain}
\bibliography{../../citations,../../Rpackages} 

\end{document}
