% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%


\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% subfigures, side by side
\usepackage{subcaption}

% hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
% \usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% Allows umlaut and non ascii characters
\usepackage[utf8]{inputenc}

% Insert blank pages
\usepackage{afterpage}
%\afterpage{\null\newpage}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

% Turn off bibliography header
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

% No page numbers
\pagenumbering{gobble}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{center}
    \large Automatic Parallelism Through R Code Analysis 

    \normalsize Clark Fitzgerald
\end{center}

%\vspace{3\baselineskip}
\hfill

For the summer of 2017 I plan to develop experimental software capable of
converting serial R programs into parallel programs using multiprocessing.
This automatic program tranformation differs from current parallel
technologies which require the user to explicitly write code for a given
parallel programming model. The software will have two major components:
performance profiling, and programmatic transformation of the code into a
parallel version.

% This is useful because it allows R programs to scale up and use the
% computational resources available on modern machines. 
% Since the
% program conversion happens automatically this allows us to build more
% intelligence into the system, freeing the user to think about the specifics
% of the problem and write higher level code, knowing that the system will do
% the best it can to find a way to parallelize the operation.

Opportunities for parallelism can be found through analysis of base R's
apply family of functions using the CodeDepends package
\cite{R-CodeDepends}. The apply family includes \texttt{lapply, apply, sapply,
tapply, by, mapply, Map, vapply, outer, by, replicate}. These are all
variants of the map reduce computational model which has
been successful for implementing large scale parallel systems
\cite{dean2008mapreduce}. 

Performance profiling measures how long the program spends executing each
part of the code. The profiling together with estimates of the overhead
associated with multiprocessing on the particular machine allow us to
determine if it's actually worth it to parallelize a given R expression.
If the parallel version is slower then the expression should
be left in serial form.

I will apply the software to several test cases including:
\begin{enumerate}
    \item Basic algorithms operating on $n \times p$ matrices
    \item Statistical simulations using \texttt{replicate()}
    \item Practical data analysis on 100's of GB of traffic sensor data 
\end{enumerate}
To pass the tests the software should be capable of generating a parallel
program that rivals the speed of a hand written parallel version. The
practical data analysis use case will be the most challenging, because it's
more difficult to profile, and the physical organization of data on disk
imposes additional constraints.
Other general challenges include respecting R's dynamic scoping rules,
avoiding nested parallelism, and handling complex control flow.

By the end of the summer I expect to produce a working prototype of the
system that can handle basic algorithms and simulations. The broader goal
is to incorporate more intelligence into the system, freeing the user to
write higher level code focused on their question rather than the specifics
of a particular parallel interface.

% As an example, the below code calculates column medians for a random matrix
% of size $n \times p$. Fix $n$ and we can empirically show that
% parallelism with 2 cores based on process forking becomes more efficient
% than the serial version for $p < P$.
% \begin{verbatim}
% x = matrix(rnorm(n * p), nrow = n)
% 
% # Calculating the column medians is 
% column_medians = apply(x, 2, median)
% \end{verbatim}
% 

\hfill

\bibliographystyle{plain}
\bibliography{../../citations,../../Rpackages} 

\end{document}
