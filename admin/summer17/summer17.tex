% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%


\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% subfigures, side by side
\usepackage{subcaption}

% hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
% \usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% Allows umlaut and non ascii characters
\usepackage[utf8]{inputenc}

% Insert blank pages
\usepackage{afterpage}
%\afterpage{\null\newpage}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}

% Turn off bibliography header
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{}{}{}

% No page numbers
\pagenumbering{gobble}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{center}
    \large Summer 2017 Research Application

    \normalsize Clark Fitzgerald
\end{center}

\emph{Indicate the dates when you will be here at UC Davis during the summer of
2017.}

I will be at UC Davis the entire summer.
   
\emph{Indicate whether you have other summer employment (TA / GSR  / Reader / Internship).
If so, please indicate dates:}

I will be a TA for STA 13 for the first summer session, June 26 - August 4.
I will also be on active NSF fellowship status.
   
\emph{Please list one faculty member who can endorse your
research activities (pertaining to point 1) during the summer of 2017.}

Professor Duncan Temple Lang

\newpage

\begin{center}
    \large Automatic Parallelism Through R Code Analysis 
\end{center}

\hfill

In summer of 2017 I plan to develop experimental software capable of
converting serial R programs into parallel programs using multiprocessing.
This automatic program tranformation differs from current parallel
technologies which require the user to explicitly write code for a given
parallel programming model. The software will have two major components:
performance profiling, and programmatic transformation of the code into a
parallel version.

% This is useful because it allows R programs to scale up and use the
% computational resources available on modern machines. 
% Since the
% program conversion happens automatically this allows us to build more
% intelligence into the system, freeing the user to think about the specifics
% of the problem and write higher level code, knowing that the system will do
% the best it can to find a way to parallelize the operation.

Opportunities for parallelism can be found through analysis of base R's
apply family of functions using the CodeDepends package
\cite{R-CodeDepends}. The apply family includes \texttt{lapply, apply, sapply,
tapply, by, mapply, Map, vapply, outer, by, replicate}. These are all
variants of the map reduce computational model which has
been successful for implementing large scale parallel systems
\cite{dean2008mapreduce}. 

Performance profiling measures how long the program spends executing each
part of the code. The profiling together with estimates of the overhead
associated with multiprocessing on the particular machine allow us to
determine if it's actually worth it to parallelize a given R expression.
If the parallel version is slower then the expression should
be left in serial form.

I will apply the software to several test cases including:
\begin{enumerate}
    \item Basic algorithms operating on $n \times p$ matrices, such as
        covariance calculations and nearest neighbors
    \item Statistical simulations using \texttt{replicate()}
    \item Practical data analysis on 100's of GB of traffic sensor data 
\end{enumerate}
To pass the tests the software should be capable of generating a parallel
program that rivals the speed of a hand written parallel version. The
practical data analysis use case will be the most challenging, because it's
more difficult to profile, and the physical organization of data on disk
imposes additional constraints.
Other general challenges include respecting R's dynamic scoping rules,
avoiding nested parallelism, and handling complex control flow.

By the end of the summer I expect to produce a working prototype of the
system that can handle basic algorithms and simulations. The broader goal
is to incorporate more intelligence into the system, freeing the user to
write higher level code that also performs better.

% As an example, the below code calculates column medians for a random matrix
% of size $n \times p$. Fix $n$ and we can empirically show that
% parallelism with 2 cores based on process forking becomes more efficient
% than the serial version for $p < P$.
% \begin{verbatim}
% x = matrix(rnorm(n * p), nrow = n)
% 
% # Calculating the column medians is 
% column_medians = apply(x, 2, median)
% \end{verbatim}
% 

\newpage

\begin{center}
    \large Automatic Parallelism Through R Code Analysis 
\end{center}

\hfill

This summer I began development on the \texttt{autoparallel} package for R
to automatically convert serial code to parallel. It's publicly available
at \url{https://github.com/clarkfitzg/autoparallel}; documentation and use
cases can be found in the \texttt{vignettes} and \texttt{inst/examples}
directories. The \texttt{CodeDepends} package is used as a dependency for
static code analysis \cite{R-CodeDepends}.

The package supports interactive function development on large
data sets. It does this by automatically distributing the data across a
cluster, and then returning a closure which evaluates serial code on each
cluster node. Code to be evaluated through this closure is analyzed to
automatically discover which functions and variables are used. Found
variables are exported to the cluster nodes so that the most current
versions are used. This allows the user to define a data analysis function,
quickly test it on a large data set, redefine the function and repeat.

Automatic parallelization happens through analysis and manipulation of
`apply()` and `lapply()` calls in the parse tree.

\hfill

\bibliographystyle{plain}
\bibliography{../../citations,../../Rpackages} 

\end{document}
