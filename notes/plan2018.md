Mon Jan  8 08:57:24 PST 2018

Winding down last year I wrote a summary of what I've been up to. In
retrospect I was too hard on R. The point of what I'm doing is not so much
to make things as efficient as possible; rather I'm trying to avoid gross
inefficiency when possible.

Now let's plan for next year. Following Norm's advice, Duncan and I need to
be on the same page about what will make a thesis.  Hmmm, I wonder if I
could have a draft completed in a year.

I just looked over Gabe Becker's thesis. The rough, high level structure
is:

1. 2 substantial motivating case studies
2. Computational model, object model
3. Proof of concept software / integration with existing software

This is about what I expected. I could follow a similar format.

# Outline

## Case studies

1. PEMS data
    - Several different analyses, ie. nonparametric, or robust
    - Generate and run on several different systems, Hive, single server
      (sufficient and limited memory) (SLURM probably too hard).
2. More numerical
    - Ethan's Vecchia approximation calculation
    - Computing sample correlation matrix (probably too simple)
3. Complete script
    - Find or make one that actually benefits from task parallelism


## Concepts / Software

1. Directed task graph in R
    - Review substantial CS literature
    - Details on generation, peculiarities to R
2. Automatically identifying and using Map-Reduce parallelism in R
    - Covers simple case of `lapply()` to `mclapply`.
    - Group by also common, can come from finding the use of `split()` in
      code
3. Pipeline parallelism 
    - Bypasses R's memory limits
    - Generate code, ie. shell commands to efficiently preprocess
    - Integrate with cursors in another type of database


## Possible Further Tasks

__Fully automate R / Hive interface__ to go from an R script that works in
memory to 
executable script. The downside is this will only apply to a quite
specific type of code:

```{R}
x = read.table(xfile.txt, col.names = ...)

fx = rbind(by(x, x$col, f))
# OR
fx = f; fx$fx = f(x$x)

write.table(fx, fxfile.txt)
```

Any slight variation from this pattern will be very difficult to handle.


__dplyr as the base language__ 
This is appealing because the unit of analysis can be just a single piped
expression that begins with reading a flat file and ends with a write. It
has a few advantages:

- More flexible than looking for the specific pattern above
- Naturally shows the pipeline parallelism
- Already generates SQL
- Can generate R to run inside Hive from the `do(.)` function

```{R}
library(dplyr)

iris %>% group_by(Species) %>% do(head(.))
```

__R in database__

I would like to meet Jeroen Ooms at UC Berkeley. Seems like he does a lot
of similar things as Duncan.

MySQL [ranks right near the top](https://db-engines.com/en/ranking) as the
most popular open source database.

There's also mongoDB for a different document store model.

How about a computation that benefits from the creation of an index?

## Non tasks

1. __Other languages__ If analysis of R code is a core part of my work then
   I shouldn't try to move everything to Julia or any other language. Some comparison of the
   semantics and features might add welcome context though.
2. __Low level infrastructure__ I've experimented with Parquet and OpenCL,
   and I could certainly spend a lot of time developing and learning at
   this level. But since it takes so long I should only do it if there's a
   very direct outcome that I need for research.
