Tue Feb 13 08:27:24 PST 2018

What should I present at JSM this summer? It would be nice to have
something more functional.

The basic premise of my project is: Other packages / systems offer a
framework or API to make your code perform better in some way. What I'm
trying to do is make a system that statically analyzes regular code and
combines this with knowledge of the data and system.

A motivating example might be taking the same input code and generating two
different versions.

### Make for task parallelism

The important idea here is analyzing and breaking down the code into
component chunks. GNU Make is essentially an easy implementation to get the
task parallelism and caching.

I'm pretty far along here based on work I did last spring.

This appeals to me on a conceptual level because it reveals in some sense
the "ideal parallelism".

A primary motivating task is propagating the change of an intermediate
computation through the graph. Probably want to use a package like R's
[storr](http://richfitz.github.io/storr/) to manage the physical storage.

There are many optimizations to be made at the implementation level, and
there are also ideas like linda spaces.

Also I should really work with [drake](https://ropensci.github.io/drake/),
ie. take advantage of that work. But it seems to have excessive
dependencies.


### Memory constrained

This really can be captured in the same thing as above. If the code chunks
only take the variables they need, then they're using the least amount of memory they
can to still stay in R's vectorized model without chunking through data on
disk.

The basic technique I have in mind to handle the memory constraints is to
keep everything on disk. This feels like a step backwards in the direction
of S, since S was always writing stuff to disk. I could actually implement
this with S3 methods on a data frame like object. This has been done
before, ie. the ff package.


### R in a database

I'd like to do something like `attach()` R to a database, so that all the
tables are available as data frames. Then automate loading to an R process,
compute and write out. But Duncan has been negative about
this idea- it's too straightforward to be research. Fair enough.


Mon Jan  8 08:57:24 PST 2018

My goal with this document is to sit down with Duncan and agree on what
constitutes a completed thesis.

Winding down last year I wrote a summary of what I've been up to after a
year of research. In retrospect I was too hard on R. The point of what I'm
doing is not so much to make things as efficient as possible; rather I'm
trying to avoid gross inefficiency when I can.

Now let's plan for next year. Following Norm's advice, Duncan and I need to
be on the same page about what will make a thesis.  Hmmm, I wonder if I
could have a draft completed in a year.

I just looked over Gabe Becker's thesis. The rough, high level structure
is:

1. 2 substantial motivating case studies
2. Computational model, object model
3. Proof of concept software / integration with existing software

This is about what I expected. I could follow a similar format.

# Outline

## Case studies

1. PEMS data
    - Several different analyses, ie. nonparametric, or robust
    - Generate and run on several different systems, Hive, single server
      (sufficient and limited memory) (SLURM probably too hard).
2. More numerical
    - Ethan's Vecchia approximation calculation
    - Computing sample correlation matrix (probably too simple)
3. Complete script
    - Find or make one that actually benefits from task parallelism


## Concepts / Software

1. Directed task graph in R
    - Review some CS literature - there's a lot!
    - Details on generation, peculiarities to R
2. Automatically identifying and using Map-Reduce parallelism in R
    - Covers simple case of `lapply()` to `mclapply`.
    - Group by also common, can come from finding the use of `split()` in
      code
3. Pipeline parallelism 
    - Bypasses R's memory limits
    - Generate code, ie. shell commands to efficiently preprocess
    - Integrate with cursors in another type of database


# Possible Further Tasks

__Fully automate R / Hive interface__ to go from an R script that works in
memory to executable script. The downside is this will only apply to a
quite specific type of code:

```{R}
x = read.table(xfile.txt, col.names = ...)

fx = rbind(by(x, x$col, f))
# OR
fx = f; fx$fx = f(x$x)

write.table(fx, fxfile.txt)
```

Any slight variation from this pattern will be very difficult to handle.


__dplyr as the base language__ 
This is appealing because the unit of analysis can be just a single piped
expression that begins with reading a flat file. It
has a few advantages:

- More flexible than looking for the specific pattern above
- Naturally shows the pipeline parallelism
- Already generates SQL
- Can generate R to run inside Hive from the `do(.)` function

```{R}
library(dplyr)

iris %>% group_by(Species) %>% do(head(.))
```

__R in database__
By this task I mean extending Duncan's work making R functions available in
a database.

I would like to meet Jeroen Ooms at UC Berkeley. Seems like he does a lot
of similar things as Duncan.

MySQL [ranks right near the top](https://db-engines.com/en/ranking) as the
most popular open source database.

There's also mongoDB for a different document store model.

How about a computation that benefits from the creation of an index?

## Non tasks

All of the tasks below take a lot of time, and they mostly don't directly
further my progress towards a completed dissertation.

1. __Other languages__ If analysis of R code is a core part of my work then
   I shouldn't try to move everything to Julia or any other language. Some comparison of the
   semantics and features might add welcome context though.
2. __Low level infrastructure__ I've experimented with Parquet and OpenCL.
3. __Other papers__ I have a draft of the PEMS paper, but I should avoid
   doing more.
4. __More classes__ I've satisfied the course requirements. Other classes
   are useful and interesting, ie. they might help me get a job, but I'm
   not worried about getting a job.
