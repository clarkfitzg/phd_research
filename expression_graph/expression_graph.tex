% A simple template for LaTeX documents
% 
% To produce pdf run:
%   $ pdflatex paper.tex 
%


\documentclass[12pt]{article}

% Begin paragraphs with new line
\usepackage{parskip}  

% Change margin size
\usepackage[margin=1in]{geometry}   

% Graphics Example:  (PDF's make for good plots)
\usepackage{graphicx}               
% \centerline{\includegraphics{figure.pdf}}

% subfigures, side by side
\usepackage{subcaption}

% hyperlinks
\usepackage{hyperref}

% Blocks of code
\usepackage{listings}
\lstset{basicstyle=\ttfamily, title=\lstname}
% Insert code like this. replace `plot.R` with file name.
% \lstinputlisting{plot.R}

% Monospaced fonts
%\usepackage{inconsolata}
% GNU \texttt{make} is a nice tool.

% Supports proof environment
\usepackage{amsthm}

% Allows writing \implies and align*
\usepackage{amsmath}

% Allows mathbb{R}
\usepackage{amsfonts}

% Numbers in scientific notation
% \usepackage{siunitx}

% Use tables generated by pandas
\usepackage{booktabs}

% Allows umlaut and non ascii characters
\usepackage[utf8]{inputenc}

% norm and infinity norm
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\inorm}[1]{\left\lVert#1\right\rVert_\infty}

% Statistics essentials
\newcommand{\iid}{\text{ iid }}
\newcommand{\Exp}{\operatorname{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{R Expression Dependency Graphs}
\date{\today}
\author{Clark Fitzgerald}
\maketitle

\begin{abstract}

    This report introduces the \textbf{expression dependency graph}, a
    directed acyclic graph capturing expression execution order implicit in
    data analysis scripts for the R language. Alternative execution models
    based on this graph are explored.

\end{abstract}

\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The idea behind code analysis is to treat the code itself as a data
structure. Then ``Programming on the language'' opens up rich possibilities.
This is an old idea, both the R and Julia language references cite Lisp
as the inspiration \cite{Rlang} \cite{bezanson2014julia}.

Compilers have used all manners of static analysis and
intermediate optimizations to create more efficient code. Interpreted
languages are much more limited in this respect. This project explores
the use of an alternative evaluation model to improve performance while
preserving language semantics.

The evaluation model for interpreted languages is simple. Each
expression of code is evaluated in the order that it appears in a text file. Informally
each expression is a line of code. This can be
viewed as a set of constraints on the evaluation order of the expressions:
\begin{enumerate}
    \item expression 1 executes before expression 2
    \item expression 2 executes before expression 3
    \item $\dots$
\end{enumerate}
What if these constraints are relaxed? Suppose expression 1 defines the variable
\texttt{x}, which is not used until expression 17. Then one has the
constraint:
\begin{enumerate}
    \item expression 1 executes before expression 17
\end{enumerate}
This can be generalized into a directed graph which we'll refer to here as
the \textbf{expression dependency graph} by considering expressions as
nodes and constraints as edges. The edges are implicit based on the order
of the statements in the code. Add an edge from $i \rightarrow k$ if
expression $k$ depends on the execution of expression $i$.  It's safe to
assume $i < k$, because expressions appearing later in a program can't
affect expressions which have already run. Hence the expression graph is
acyclic, i.e. a DAG.

Scheduling execution based on the expression graph allows some expressions to execute in
parallel. For example, the following adjacent lines are independent, so
they can be computed simultaneously:

\begin{verbatim}
sx = sum(x)
sy = sum(y)
\end{verbatim}

Mathematically, the standard evaluation model is a total ordering on the
set of expressions in the code. The dependency graph is a partial ordering.

\section{Literature Review}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\label{sec:lit}

The expression graph proposed above is similar to 
use-definition and definition-use chains. A definition-use chain consists
of all expressions using a variable following the definition of that
variable. This amounts to a subset of the edges in the expression graph,
since it's possible that expressions depend on each other without
variables. For example, consider the following R code to save a plot to a
pdf:
\begin{verbatim}
pdf("xy.pdf")
plot(x, y)
title("x and y")
dev.off()
\end{verbatim}
These expressions depend on each other, but only \texttt{plot(x, y)} uses variables.

The use-definition chain has been around since at least 1978
when it was used to remove dead (unused) code \cite{kennedy1978use}.
Code usefulness is defined recursively; a computation is useful if the result is
used later by another computation. This is combined with the ``base case''
of usefulness, a set of operations considered
intrinsically useful. This author considers calls to subroutines and branch
test instructions as intrinsically useful. For data analysis we might take
this base case and tweak it a little- define expressions in a data analysis
script as intrinsically useful if they have a side effect, for example
saving data to disk.

More general than the use-definition chain is the program dependence graph (PDG)\cite{ferrante1987}:
\begin{quote}
    A PDG node represents
    an arbitrary sequential computation (e.g., a basic block, a
    statement, or an operation). An edge in a PDG represents
    a control dependence or a data dependence. PDGs do not
    contain any artificial sequencing constraints from the
    program text; they reveal the ideal parallelism in a
    program. \cite{sarkar1991automatic} 
\end{quote}
\cite{sarkar1991automatic} goes on to make the practical distinction between ideal
parallelism and useful parallisism. Overhead implies that the two often
differ.
The expression graph proposed here differs from the PDG since it allows the permutation of
operations in a basic block.

The hierarchical task graph (HTG) was introduced in \cite{girkar1992automatic}
to detect task parallelism in source code for use in compilers.
Similar to the others, it examines the control flow for a fine grained
parallelism. They allow `compound nodes' containing nested HTG's.
\cite{cosnard1995automatic} describe constructing a task graph based on
annotating the source code the program.
\cite{adve2004parallel} presents a model for predicting the run time of
programs based on a task graph. 

The literature cited in this section focuses primarily on compiled
languages along with careful analysis of control flow. Most examples and
applications presented along with these papers are for well-defined
algorithmic problems. These algorithmic problems are often quite different
than a high level data analysis script which may call down into several
different algorithms.

\section{Languages Requirements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We can consider building the expression graphs described above for languages that are
\begin{itemize}
    \item open source
    \item used for data analysis
    \item high level
    \item interpreted
    \item enable metaprogramming
\end{itemize}
Metaprogramming warrants more explanation. This refers to programmatically
inspecting and potentially modifying code from within the language.  It is
needed to determine when variables are created and used, among other
things.  The current popular languages satisfying these requirements are
Python, Julia, and R.

The basic unit to analyze is a single code expression.
Most expressions use symbols, aka variables or names.
An expression may do any combination of the following actions:
\begin{enumerate}
    \item Define new symbols: \texttt{x = 10}
    \item Use existing symbols: \texttt{sin(x)}
    \item Redefine existing symbols: \texttt{x = 20}
\end{enumerate}

\begin{table}[]
\centering
    \caption{Sorting \texttt{x} in place}
\label{tab-sort}
\begin{tabular}{ll}
    \textbf{language} & \textbf{code}        \\
\hline
    Python   & \texttt{x.sort()}    \\
    Julia    & \texttt{sort!(x)}    \\
    R        & \texttt{x = sort(x)}
\end{tabular}
\end{table}

Table \ref{tab-sort} shows idiomatic code to sort a numeric vector.
The Julia and Python methods modify their arguments in place.  From a
computational standpoint this is great, since it allows the implementations
to use more space efficient sorting techniques. However, from a code
analysis standpoint this behavior is undesirable, since it means that we
need to assume generally that every method call in these languages both
uses and potentially modifies the object. Since data analysis scripts mainly consist of
function and method calls this will excessively constrain the problem.

It may be possible to recursively examine all functions and methods which
are used, but this is not ideal for a couple reasons. First, it would
require analyzing the underlying library code, which is orders of magnitude
more code than what the user has written. Second, eventually we'll get to
compiled code which requires totally different methods. For example, C code
parsed with LLVM
can be used to programmatically generate use-definition chains
\cite{lattner2004llvm}.

Hence functional programming and pass by value semantics make constructing
graphs much more feasible. The R language is ideal in this respect.

\section{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Bengtsson's \texttt{future} package provides a mechanism for
asynchronous assignment and evaluation of R expressions \cite{R-future}. Once the
expression graph is created it might be possible to use similar mechanisms
for evaluation.

Hester's covr package \cite{R-covr} checks unit test coverage of R
code. It is a practical example of computing on the language,
programmatically modifying the code by recording calls as they are made.
He pointed out the necessary distinction between AST's, parse trees, and
``lossless syntax trees''.
\footnote{\url{https://news.ycombinator.com/item?id=13628412}} To inject
code into a script one needs to be very careful to preserve structure lost
after parsing, ie.  comments and formatting. This is a non-trivial task.

A similar strategy of recording calls should work to collect timings and
resource usage for each expression and for functions called with various
data sizes. Then we can potentially use this to change the execution when
it becomes efficient to do so.  This resembles something like Profile
Guided Optimization (PGO). 

Maybe a simpler way to do this is to just use the built in profiler. Use
the results to determine whether parallelization is worth it, and maybe set
some bounds for expected performance changes if one uses various forms of
parallelism.  Statistical methods could potentially be used for this.
Run it many times, collecting profiling results for various values and use
this as the training data to produce a rule such as: if $n > 10^6$ then run
it as multicore.

The vignette in Tierney's \texttt{proftools} package has some nice examples
of visualizing profiling data \cite{R-proftools}. The call graphs and
related visualizations are conceptually similar to what might be done with
the expression graph.  \texttt{profvis} integrates with the IDE to indicate
the actual line of source code along with the related timing info
\cite{R-profvis}.

Xie's \texttt{knitr} facilitates reproducible computations for
chunks of code in Rmarkdown documents \cite{R-knitr}. One feature it enables is caching,
ie. it doesn't need to run a chunk of code if nothing has changed. One can
manually specify the chunk dependencies by relative or absolute indices,
ie. -2 for the chunk 2 blocks in front of the current chunk, or 1 for the
first chunk. This seems unreliable because it requires the user to
accurately infer the dependency information, and it doesn't automatically
adjust if one inserts new chunks in the document.

Knitr also has an \texttt{autodep} option to infer this dependency
information. This works by comparing the global variables existing before
and after running the code in each chunk. It stores this information in
special files. So it doesn't use any static analysis of the code.

But the structure of knitr blocks here is actually very appealing- this is
a great use case for the parallelism. Why not evaluate the chunks in
parallel if possible? The case for Jupyter notebooks is similar, but would
require an equivalent code dependency graph for Python.

\section{Graph Construction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To start off we make two assumptions on the program. First, assume it's
correct, meaning that it will run sequentially without errors.
Second, every expression should be strictly necessary. This can be achieved
through a preprocessing step removing dead code.

The expression graph and related data structures are related to the parsed
script, referred to as the \textbf{parse tree}. The expression
graph is a function of the parse tree. Since the parser
doesn't care about non significant white space and comments,  different
scripts can produce the same parse tree.  Many parse trees can give rise to
the same expression graph. For example, the expression graph shouldn't care if one uses
\texttt{=} or \texttt{<-} for assignment. Nor will it care about the
ordering of two adjacent lines binding a symbol to a literal constant:

\begin{verbatim}
a = 1
b = 2
\end{verbatim}

Therefore we lose information by converting from a parse tree to an
expression graph.

``Expression'' here refers more specifically to top level expressions, the children of
the root node in the parse tree. For example, \texttt{for} and
\texttt{while} loops are function calls in R, and the expression graph
treats them the same as any other function calls. This means that we do not
currently examine control flow at all. It is possible to examine
expression graphs recursively; conceptually the ideas will be similar.

Figures \ref{fig:ast} and \ref{fig:codegraph} illustrate the 
parse tree and expression dependency graph for the four lines of code in
listing \ref{list:ab}.  Edges 1 and 3 in figure \ref{fig:codegraph} represent the
respective uses of the variable \texttt{n} and \texttt{x}.  Edge 2 comes
from the redefinition of \texttt{n}.  Edge 5 propagates the most recent
definition of \texttt{n}.  The least obvious is edge 4, which is necessary
to respect R's lexical scoping semantics since \texttt{x <- rnorm(n)} uses
the first definition of \texttt{n}. The general rule here is that all
statements using one version of the variable \texttt{n} must execute before \texttt{n}
can be redefined.

The dashed edges in figure \ref{fig:codegraph} are redundant for representing
expression dependence given the other edges. Indeed, the code in
listing \ref{list:ab} must run sequentially. One may wish to remove such
redundant edges, especially for visual presentation of a larger program.

\lstinputlisting[language=R, caption=Simple script, label=list:ab]{../experiments/ast/ab.R}

\begin{figure}
\centering
\begin{subfigure}{.6\textwidth}
    \centering
    \includegraphics[width=.8\linewidth]{../experiments/ast/ast.pdf}
    \caption{Parse tree}
    \label{fig:ast}
\end{subfigure}%
\begin{subfigure}{.4\textwidth}
  \centering
  \includegraphics[width=.8\linewidth]{../experiments/ast/codegraph.pdf}
  \caption{Expression dependency graph}
  \label{fig:codegraph}
\end{subfigure}
\caption{Different representations of the script in Listing \ref{list:ab}}
%\label{fig:test}
\end{figure}

The existence of some edges may depend on conditional statements which can't
be known until run time. In this case the conservative and correct way to
handle the situation is to add the edges in question, because adding edges
cannot constrain the problem beyond the constraints imposed by sequential
execution. For example, in the following code one assumes that the
expression \texttt{x <- 10} will run.

\begin{verbatim}
# coinflip() randomly returns TRUE or FALSE
if(coinflip()){
    x <- 10
}
\end{verbatim}

\section{Task Based Parallelism}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

It's reasonable to try to improve code performance if slow speed affects
many users. For scripts, superficially it might appear that few people are
affected. Indeed, if a researcher writes one script that takes a couple
minutes to run, and they run it a couple times then it doesn't matter much,
and there's no point in attempting to accelerate it.  However, 
scripts can be used in more serious ways.  For example, scripts
can be used as Extract Transform Load (ETL) tools that run as batch jobs
every day or every hour. Then the script runs many times, so it's important
to realize more performance. For ease of use and maintainability it's nice
to have automatic tools to accelerate the performance. Then one can write
simple scripts and leverage a powerful tool to gain parallelism. This is
preferable to maintaining many complex scripts since it requires less
expert knowledge.

As of 2009, most efforts to parallelize R have focused on the lower level
programming mechanisms \cite{schmidberger2009state}. These needed to be in
place before any higher level automatic detection could be built and
function.

Let $k$ be the number of cores on a machine.
To accelerate code using this single machine the best possible case is if
we can keep all $k$ cores busy at once. This will happen if there are $nk$
expressions which can be independently scheduled for some positive integer
$n$. 
On a two core machine that script might look something like:

\begin{verbatim}
# These could be run in parallel
a = long_running_func()
b = long_running_func()
\end{verbatim}

The worst possible case is if the second long running computation depends
on the first, and everything else depends on the second. Then it must run
in serial so parallelism can't help.

\begin{verbatim}
a = long_running_func()
b = long_running_func2(a)  # depends on a
# Now perform many operations on b
\end{verbatim}

\subsection{Static Execution}
\label{sec:static}

If overhead and expression run time are approximately known then the
question of optimal execution for the complete expression dependency graph
can be framed as a scheduling optimization problem and solved statically.
The objective function to minimize is the total wall clock time to complete
execution. Constraints come from the dependency graph and that at most $k$
cores may be active at one time.

To consider an alternative evaluation model the overhead required to
parallelize an expression should require less time than running the
expression itself. Rounding to orders of magnitude, here are some rough
times for reference executing on a modest machine. Simple R expressions
take $10^{-7}$ seconds to evaluate. Using a system level parallel fork
requires $10^{-3}$ seconds of overhead. Evaluation on an existing local
socket cluster takes $10^{-4}$ seconds. Then either of these well
established methods for parallelism in R won't become efficient until the
code under evaluation takes on the order of $10^{-3}$ seconds.
These timings include latency for interprocess communication on a single
machine. Bandwidth is also an issue, since serializing large amounts of
data between processes, i.e. millions of floating point numbers, will impact
performance. For example, listing \ref{list:overhead} squares each element
of a vector of one million floating point numbers. Memory transfer overhead causes
a parallel evaluation to take more than an order of magnitude more time
than the single threaded serial version. 

Technical solutions such as threading and shared
memory have the potential to reduce these sources of overhead.
Issues of latency and bandwidth generally become more complex for different
architectures such as distributed systems and GPU's
\cite{matloff2015parallel}.

\subsection{Dynamic Execution}
\label{sec:dynamic}

Alternatively a dynamic execution model can be used. This relies on a
master / worker architecture. The reference version could be a multicore
system which forks to evaluate expressions.  At a high level, the master
runs an event loop that pops expressions from the top of the expression
dependency graph.  This approach is appealing because it dynamically
balances the load among workers.

Here's an algorithm: Insert the artificial node 0 representing the beginning
of the script, so that nodes without parents now have node 0 as a parent. Mark
these direct descendants of node 0 as ready. Let the workers begin evaluating
these expressions.

\begin{enumerate}
    \item Event loop checks to see if any are done.
    \item Expression $e_i$ finishes and the results are available again
        on master.
    \item For each expression $e_j$ which depends directly on $e_i$: check
        if $e_j$ has no other existing parents then mark it as ready.
    \item Remove $e_i$ from the graph.
    \item Free workers begin executing any of the nodes that are marked as ready.
\end{enumerate}

There's some nuance here by trying to keep each of $k$ workers as busy as
possible while still respecting the constraints. I.e. there may be
bottlenecks where only one worker can be active, but after that all the
others can go.

This algorithm could also be refined into a priority queue by keeping the ready
nodes in a heap, with the values determining the heap order as the number
of expressions that depend on that expression, directly or indirectly. This
is a little naive- it would be better to have timings of the code and do
something more optimal in terms of reducing run time.

The process forking every time will be quite inefficient. The more intelligent
thing to do is `pipeline' the operations, and send whole related blocks of
expressions to individual processes to evaluate.

\section{Challenges}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

R's flexibility makes code analysis challenging in some cases.

\subsection{Reproducibility}

Reproducing random streams allows one to perform the exact same random
computation. This is useful for investigating anomalous simulations, and
for many other cases. The R documentation for \texttt{parallel::mcparallel}
explains how this works in the case of parallel forking:

\begin{quote}
     The behaviour with ‘mc.set.seed = TRUE’ is different only if
     ‘RNGkind("L'Ecuyer-CMRG")’ has been selected.  Then each time a child
     is forked it is given the next stream (see ‘nextRNGStream’).  So if
     you select that generator, set a seed and call ‘mc.reset.stream’ just
     before the first use of ‘mcparallel’ the results of simulations will
     be reproducible provided the same tasks are given to the first,
     second, ...  forked process.
\end{quote}

If a dynamic model is used as described in section \ref{sec:dynamic}, then
reproducing the exact computation may not be possible, since the code may not execute in the
same order. However, if this level of reproducibility is important then one
can force it through manually seeding the functions that matter. For
example:
\begin{verbatim}
critical_random_func()
\end{verbatim}
becomes
\begin{verbatim}
{set.seed(123); critical_random_func()}
\end{verbatim}

\subsection{Dynamic Evaluation}

Correct, legitimate code can be written that depends on the results of
dynamic evaluation. Indeed, some symbols may not currently exist. This is
more common inside package code that defines and uses many functions, and
less common in scripts.  Here's an example:

\begin{verbatim}
f = function() 0        # 1
g = function() f() + 1  # 2
f = function() 10       # 3
g()                     # 4
\end{verbatim}

The last line returns 11, since it uses the most recent version of \texttt{f()}.
An expression dependency graph that does not correctly handle dynamic
evaluation here will consist of these edges:

\begin{verbatim}
1 -> 2, 1 -> 3, 2 -> 4
\end{verbatim}

So the statements could be written in the following order, which respects
the partial order:

\begin{verbatim}
f = function() 0        # 1
g = function() f() + 1  # 2
g()                     # 4
f = function() 10       # 3
\end{verbatim}

In this case the call to \texttt{g()} will incorrectly return 1 instead of 11.
Hence there is a ``hidden'' dependency implicit here: \texttt{3 -> 4}.
This comes back to lexical scoping rules, since we need to look up the
correct \texttt{f()}.

One possible way to get around this is to recursively inline all user
defined functions, and then perform the expression dependency analysis. One
substitutes the bodies of \texttt{g(), f()} so the code presented above
becomes simply \texttt{10 + 1}. This also has the effect of removing user
defined functions from the expression graph.

\subsection{Mutable Objects}

Environments and reference class objects in R are mutable. They are a
special case and must be handled carefully. One way to handle them is to
treat any access of one of these objects as a redefinition. This resembles
the conservative approach one would have to take for Python or Julia
methods, and could result in a similar outcome of having too many
constraints.

With environments one could refer to all variables specifically through
(environment, symbol) pairs. Through these pairs all variables in
environments essentially act like regular variables, and variables in the
global environment are just a special case.

\subsection{Non Standard Evaluation}

The R code \texttt{lm(y \textasciitilde x, data = d)} is ambiguous because
\texttt{x, y} may be global variables or they may be columns in the data
frame \texttt{d}. CodeDepends doesn't detect the dependency on variables
\texttt{y, x} because of non standard evaluation (NSE). For interactive use
NSE can be convenient, but it comes at the cost of referential transparency
\cite{wickham2015advanced}. The programming model in this case is no longer
functional, which makes code dependency analysis much more difficult.

A related case to NSE is user code that operates directly on the language
through expression manipulation. At this point we're metaprogramming on
code that does metaprogramming, which sounds like a dangerously bad idea.
We're better off leaving it alone or operating on code that it generates,
if possible.

As before, the conservative approach is to add edges and dependencies when in
doubt. So in the code above one assumes that all of the variables
\texttt{x, y, d} will be used.

\subsection{Side Effects}

As mentioned in section \ref{sec:lit}, it's difficult to recognize the
dependence structure with plotting commands. One way to handle this is to
use a preprocessing step to collapse all steps modifying the graphics
device into one block of code. This can be implemented by scanning the
script and adding braces around the lines of code that open and close
graphics devices, for example:

\begin{verbatim}
{   # Added by preprocessor
pdf("plot.pdf")
... # plot(), title(), text(), etc.
dev.off()
}   # Added by preprocessor
\end{verbatim}

This block will then be executed together as one top level expression.

\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This work introduced the expression dependency graph and presented an
alternative execution model for R code based on dependency analysis.
From my experience examining and running data analysis scripts, 
data parallelism is far more important than task parallelism. Indeed, in
the course of examining real world scripts for this I couldn't find any
examples where this approach would result in speedup justifying the
overhead.  However, if lighter weight threads or shared memory were
possible in R then the relative overhead could be made orders of magnitude
smaller, and this task based approach to parallelism should be revisited.

Future work will explore other uses for the expression graph beyond
parallelism. For example, the expression graph could be used for
educational purposes or for debugging.

On a broader note, this is about embedding more intelligence into the 
system. Many languages have mechanisms or third party tools for explicitly
requesting asynchronous evaluation. These typically require changing the
code. This introduces complexity, makes maintenance more difficult, and
makes the code less portable.  It's more convenient to have one version of
the code which can be passed to a system which can analyze the code and
``do the right thing'', adjusting to different platforms, work loads, and
data sizes on the fly. 

\newpage
\appendix

\section{Definitions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The following material is from the R Language Reference \cite{Rlang}.

``A \textbf{statement} is a
syntactically correct collection of tokens.'' Less formally one can think of
a statement as a single line of code. The `line' rule doesn't always hold in practice,
since a semicolon can put two statements on one line, and a single
statement may span multiple lines.

\begin{verbatim}
# Two statements on line
a = 1; b = 2

# One statement on multiple lines
plot(x,
     y)
\end{verbatim}

\textbf{symbol} is a variable name such as \textbf{a, b} above.  The words
symbol, variable, and name are used interchangeably. For consistency I'll
stick with symbol.

Assignment is the binding of a symbol to an R object.

``An \textbf{expression} contains one or more statements.'' Expression objects
in the langauge contain parsed but unevaluated statements. 

Statements can be grouped together using braces to form a \textbf{block}.
Since expressions can be nested, we can consider a block just a special
type of expression.

\begin{verbatim}
{
a = 1
b = 2
}
\end{verbatim}

\section{CodeDepends}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

CodeDepends is the underlying package which generates the expression
dependency information \cite{R-CodeDepends}.

The parsing uses an object oriented wrapper around R's builtin \texttt{parse()}
which handles different file types ie. script or dynamic documents among other arguments.
This doesn't directly expose code comments.
At first glance it seems CodeDepends uses the tokens
more indirectly, through functions like \texttt{is.name()}. 

The workhorse functions are in \texttt{CodeDepends.R}. Overall, the
approach resembles \texttt{codetools::walkCode} as discussed in
\cite{chambers2016extending}. Essentially it walks the parse tree, calling
a function to collect usage information.

When analyzing a single expression, first a 
collector object is created with \texttt{inputCollector()}. This is a
closure that maintains a list of everything in the expression that has been seen
so far: files, variables, function calls, etc. It returns a list of
functions to update the data in the closure.

\texttt{getInputs.language()} takes a collector object and
recurses through expressions until it finds the leaf nodes which can be functions, calls, assignments,
names, literals, or pairlists. Upon finding one of these leaf nodes it
calls the collector object. 
Many special cases of functions are handled in functionHandlers.R, such as
\texttt{\$, rm, for} as well as non standard evaluation. Special attention seems
to have been paid to dplyr operations.


\section{LLVM}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

LLVM has some functionality for working with these use / def chains
\cite{Lattner2004}. For example:
\url{http://llvm.org/docs/ProgrammersManual.html#iterating-over-def-use-use-def-chains}.
Following their terminology, an object of class \texttt{Value} is used by
potentially many objects of class \texttt{User}. The \emph{def-use} chain
is the list of all \texttt{Users} for a particular \texttt{Value}. Think
of this as all the places in the code where this variable propagates. In
contrast, the \emph{use-def} chain is the list of all \texttt{Values} for a
particular \texttt{User}. This is all the inputs to a newly
created object. There's room for both of these chains to be expanded recursively.

In this example the def-use chain for \texttt{x} is only \texttt{[y]}, but the
recursive one is \texttt{[y, z, z2]}. The use-def chain for \texttt{y} is
\texttt{[z, z2]}.

\begin{verbatim}
x = 10
y = x + 5
z = y + 2
z2 = y + 100
\end{verbatim}

We might be able to use this along with R code to determine if an R
function calling C code is pure.

\lstinputlisting[language=R, caption=Overhead caused by memory transfer,
label=list:overhead]{overhead.R}


\bibliographystyle{plain}
\bibliography{citations} 

\end{document}
