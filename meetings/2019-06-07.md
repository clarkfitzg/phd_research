Fri Jun  7 14:43:14 PDT 2019

Given the code, data, and platform, here's what we need to do:

1. Collect / infer all the information about the structure of the data and computations.
    For example, for data frames, is the data organized by some column, and does the code split by one or more columns?
2. Enumerate all the different strategies for organizing the computations and distributing the data among the workers.
    Estimate how long each one will take.
3. Rewrite / generate code that uses one of these strategies.

Nick suggested that I first write very specifically what information the second step requires before implementing the first step.
This will help me focus and only implement things that are necessary.

Regarding step 1, Duncan would like to see:

A function that infers information about the code.
It should take in a script, say `file.R`, perhaps a list of vectorized functions, and a list of files.
It should output all the possible ways to organize the data such that we can do the computation.


## Data Locality

For efficiency, we want to avoid moving the data whenever possible.
This means that if we split the data into chunks, we want to do as many operations on those chunks as we can.
Here's a simple example:
```{r}
# Suppose we have a huge data frame `d`, i.e. billions of rows
y = cos(d$x)
z = sin(exp(y)) + d$x
m = max(z)
print(m)
```

The lines `y = cos(d$x)` and `z = sin(exp(y)) + d$x` are vectorized.
If `d` is distributed among several workers, then we would like these lines to run in parallel.

My approach has been to literally expand the code into as many chunks as we have, so that we can use a general task scheduling algorithm that knows about data locality.
So if `d` is three chunks, `d1, d2, d3`, I would expand the code into
```{r}
y1 = cos(d1$x)
y2 = cos(d2$x)
y3 = cos(d3$x)
```
And so on, for every vectorized statement, collecting the objects up before calling general functions.

I have been getting hung up on the implementation details of expanding statements, and am worried about how the implementation will generalize.

Duncan made the point that we don't need to rewrite the code at all to run the vectorized parts in parallel.
That is, each of the workers can load in their own chunk of the original complete object `d`, assign it to a local variable called `d`, and just run the vectorized parts of this code without modification.
The objects don't have to be identified with a unique name, because they can be identified by which worker sends a particular object.
For example, if worker 1 sends an object, then that's chunk 1.
