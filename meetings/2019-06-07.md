Fri Jun  7 14:43:14 PDT 2019

Given the code, data, and platform, here's what we need to do:

1. Collect / infer all the information about the structure of the data and computations.
    For example, for data frames, is the data organized by some column, and does the code split by one or more columns?
2. Enumerate all the different strategies for organizing the computations and distributing the data among the workers.
    Estimate how long each one will take.
3. Rewrite / generate code that uses one of these strategies.

Nick suggested that I first write very specifically what information the second step requires before implementing the first step.
This will help me focus and only implement things that are necessary.

Regarding step 1, Duncan would like to see:

A function that infers information about the code.
It should take in a script, say `file.R`, perhaps a list of vectorized functions, and a list of files.
It should output all the possible ways to organize the data such that we can do the computation.

Both Duncan and Nick weren't seeing why I should start with the data split into files, as one of the more interesting questions is how to distribute the data among workers in the first place.
For a single GROUP BY, the ideal case is of course when each group is contained in exactly one file, and that's where I started.
Reflecting on this now, I see a way to assign the groups to workers for a single GROUP BY operation such that they are more local.
I was mistaken in thinking there's only one way to do it.
If we know the original distribution of $g$ groups among many original data files, then the problem is the same as the two different GROUP BY computations that I considered last month.
I should develop this further, and get it working.


## Data Locality

For efficiency, we want to avoid moving the data whenever possible.
This means that if we split the data into chunks, we want to do as many operations on those chunks as we can, and avoid transferring intermediate results.
Here's a simple example:
```{r}
# Suppose we have a huge data frame `d`, i.e. billions of rows
y = cos(d$x)
z = sin(exp(y)) + d$x
m = max(z)
print(m)
```

The lines `y = cos(d$x)` and `z = sin(exp(y)) + d$x` are vectorized.
If `d` is distributed among several workers, then we would like these lines to run in parallel.

My approach has been to literally expand the code into as many chunks as we have, so that we can use a general task scheduling algorithm that knows about data locality.
So if `d` is three chunks, `d1, d2, d3`, I would expand the code into
```{r}
y1 = cos(d1$x)
y2 = cos(d2$x)
y3 = cos(d3$x)
```
And so on, for every vectorized statement, collecting the objects up before calling general functions.

I have been getting hung up on the implementation details of expanding statements, and am worried about how the implementation will generalize.

Duncan made the point that we don't need to rewrite the code at all to run the vectorized parts in parallel.
That is, each of the workers can load in their own chunk of the original complete object `d`, assign it to a local variable called `d`, and just run the vectorized parts of this code without modification.
One way to do this is to gather all the vectorized code into a function.
The objects don't have to be identified with a unique name, because they can be identified by which worker sends a particular object.
For example, if worker 1 sends an object, then that's chunk 1.
_I wrote about this last year with what I was writing on chunk fusion.
IIRC, I went away from it because it won't work as well with task parallelism._
