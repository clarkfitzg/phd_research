Fri Jun  7 14:43:14 PDT 2019

Duncan would like to see:

A function that infers information about the code.
It should take in a script, say `file.R`, perhaps a list of vectorized functions, and a list of files.
It should output all the possible ways to organize the data such that we can do the computation.

Given the code, data, and platform, here's what we need to do:

1. Collect / infer everything that influences how we might organize the computations and distribute the data among the workers.
2. Enumerate all the different strategies, and how long each one will take.
3. Rewrite / generate code that will use one of these strategies.


## Data Locality

For efficiency, we want to avoid moving the data whenever possible.
This means that if we split the data into chunks, we want to do as many operations on those chunks as we can.
Here's a simple example:
```{r}
# Suppose we have a huge data frame `d`, i.e. billions of rows
y = cos(d$x)
z = sin(exp(y)) + d$x
m = max(z)
print(m)
```

The lines `y = cos(d$x)` and `z = sin(exp(y)) + d$x` are vectorized.
If `d` is distributed among several workers, then we would like these lines to run in parallel.
Duncan's point is that we don't need to rewrite the code at all for this to happen.
That is, each of the workers can load in their own chunk of the original complete object `d`, assign it to a local variable called `d`, and just run the vectorized parts of this code without modification.

My approach has been to literally expand the code into as many chunks as we have, so that we can use a general task scheduling algorithm that knows about data locality.
So if `d` is three chunks, `d1, d2, d3`, I would expand the code into
```{r}
y1 = cos(d1$x)
y2 = cos(d2$x)
y3 = cos(d3$x)
```
And so on, for every vectorized statement, collecting the objects up before calling general functions.
