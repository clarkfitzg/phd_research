Fri Aug  9 17:11:23 PDT 2019

## TODO

0. Fill in the schedule for a plan of what to finish week by week until September 20th deadline for draft.
1. Write intro / motivation chapter sketched here..
2. Get the vector scheduler working, omit the reduce for the time being.
3. Handle detecting external data, for example the `x = read.csv("x.csv")` in this code.
4. Document the task scheduler in thesis, why doesn't it work well?

------------------------------------------------------------

In the abstract, clarify what's in there, and what we'll do.

Err on the side of writing a user manual versus something that's excessively academic- i.e. don't try to make it look like a fancy CS paper.
The focus should be on the concepts.
We can add and generalize that kind of stuff later.

Do write chapter abstracts that say what are the claims, and a quick summary.
These may not go directly into the final product, but they will be useful for focusing.

Big questions:

- What's the class of problems that we are solving?
- What are the assumptions, where are we starting from?
    Specifcially, the vectorized example needs this.

Add more content with experiments and numerical times showing improvement.

We don't have to include everything.
Thesis is not a diary of everything you did in grad school.

Think carefully about what each section means, and why it's there.
Specifically, in each section:

- What's the purpose?
- What does it address?
- What are the conclusions?
- Why is it important?

Idea for incrementally growing a data example:

1. single files
2. multiple files
3. URL's
4. DBMS

We _need_ to be able to handle the cases where the data is not already split, and when the data description is in the code, i.e. starting from `x = read.csv("x.csv")$x1`.
It's fine and good to separate the external data dependency detection into a separate step, say to go in CodeAnalysis.


## Incremental example

Write the first chapter to provide motivation as a series of incrementally growing examples, meaning we just change one thing.
An assortment of problems, and here's how we do it.
Start with simple serial code, and several kinds of data.

Suppose a person walks into the DSI with some code, some data, and wants you to make it faster.

The point is to show that the answer, the 'right way' to evaluate the code, depends on many things- it's different, often completely different. 
The difference is like a discontinuous function- all of a sudden we switch to a completely different solution.

Ideas for example problems:

1. Combining vectorized statements, i.e. `y = f(z); z = g(y); a = h(z)`
2. XML / pdf document processing
3. Simulations

For a simple example, I can start with reading a text file, something like `scan`, or `as.numeric(readLines(...`.

Another more realistic example than the `max(sin(x))` might be to convert things to dates, and find the range of the dates.
Here's how it could progress:

1. It's a text file with one column `d = readLines(...); x = as.date(d); range(x)`
2. The date is in the 3rd column, and we can pick out the column.
3. Add a filter, say past a certain date.
4. Put the reduce into the solution, so we calculate the range on each worker.
5. Distributed machines, figure out how to send data
    - Hierarchical workers, several on each physical machine
    - Heterogeneous workers, different processing speed
6. Data in URL
7. GPU, suppose we have a kernel

For the examples stick to high level / semi pseudo code.
Mention the 'wrong way', so you try to parallelize something this way and it now takes twice as long, say.
