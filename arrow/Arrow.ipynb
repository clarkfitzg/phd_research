{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from the top of this page: http://anson.ucdavis.edu/~clarkf/\n",
    "\n",
    "I used conda to install pyarrow: https://anaconda.org/conda-forge/pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet\n",
    "\n",
    "Summarizing the [parquet documentation](https://parquet.apache.org/documentation/latest/):\n",
    "\n",
    "- Goal is interoperability across Hadoop ecosystem\n",
    "- Compression can be specified per column\n",
    "- Handles complex nested data structures (similar to XML)\n",
    "- Follows the 2010 Google Dremel paper\n",
    "\n",
    "Column chunks are guaranteed to be contiguous within a row group. Here's a hierarcy for a table with 2 columns. There's also metadata at each level.\n",
    "```\n",
    "File\n",
    "    Row Group 1\n",
    "        Column Chunk 1\n",
    "        Column Chunk 2\n",
    "    Row Group 2\n",
    "        Column Chunk 1\n",
    "        Column Chunk 2        \n",
    "    Row Group 3\n",
    "        ...\n",
    "```\n",
    "Docs recommend configuring 1GB row group sizes, corresponding to 1 GB HDFS blocks. \n",
    "\n",
    "- Row based MapReduce runs in parallel across Row Groups\n",
    "- IO runs in parallel across column chunks.\n",
    "\n",
    "Advantages include pushdowns for queries, ie. only reading 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the dataset as below is cheap because it only reads the metadata.\n",
    "\n",
    "Metadata is represented with Apache Thrift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pems = pq.ParquetDataset(\"/Users/clark/data/pems/pems_sorted/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema and partitioning from the database (Hive) is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.ParquetSchema object at 0x1149141c8>\n",
       "timeperiod: BYTE_ARRAY UTF8\n",
       "flow1: INT32\n",
       "occupancy1: DOUBLE\n",
       "speed1: DOUBLE\n",
       "flow2: INT32\n",
       "occupancy2: DOUBLE\n",
       "speed2: DOUBLE\n",
       "flow3: INT32\n",
       "occupancy3: DOUBLE\n",
       "speed3: DOUBLE\n",
       "flow4: INT32\n",
       "occupancy4: DOUBLE\n",
       "speed4: DOUBLE\n",
       "flow5: INT32\n",
       "occupancy5: DOUBLE\n",
       "speed5: DOUBLE\n",
       "flow6: INT32\n",
       "occupancy6: DOUBLE\n",
       "speed6: DOUBLE\n",
       "flow7: INT32\n",
       "occupancy7: DOUBLE\n",
       "speed7: DOUBLE\n",
       "flow8: INT32\n",
       "occupancy8: DOUBLE\n",
       "speed8: DOUBLE\n",
       " "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nulls\n",
    "\n",
    "Quite a bit different than using IEEE NaN or a special bit pattern.\n",
    "\n",
    "> Nullity is encoded in the definition levels (which is run-length encoded). NULL values are not encoded in the data. For example, in a non-nested schema, a column with 1000 NULLs would be encoded with run-length encoding (0, 1000 times) for the definition levels and nothing else.\n",
    "\n",
    "Much better for sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onefile = pq.ParquetFile(\"/Users/clark/data/pems/pems_sorted/station=402260/part-r-00000-ddaee723-f3f6-4f25-a34b-3312172aa6d7.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onefile.num_row_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x11694d598>\n",
       "  created_by: parquet-mr version 1.6.0\n",
       "  num_columns: 25\n",
       "  num_rows: 2575\n",
       "  num_row_groups: 1\n",
       "  format_version: 1.0\n",
       "  serialized_size: 3389"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onefile.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'org.apache.spark.sql.parquet.row.metadata': b'{\"type\":\"struct\",\"fields\":[{\"name\":\"timeperiod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow1\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy1\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed1\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow2\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy2\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed2\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow3\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy3\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed3\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow4\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy4\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed4\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow5\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy5\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed5\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow6\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy6\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed6\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow7\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy7\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed7\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow8\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy8\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed8\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onefile.metadata.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Arrow\n",
    "\n",
    "> Powering Columnar In-Memory Analytics\n",
    "\n",
    "A bold claim... R, Python (Numpy), and Julia all compete in this space.\n",
    "\n",
    "Source: [Arrow Docs](https://arrow.apache.org/)\n",
    "\n",
    "Essentially Arrow is a specification for a memory layout, along with high performance C++ and Java implementations.\n",
    "\n",
    "My initial experiments using it for shared memory were positive. Shared memory would be very nice for interoperability between different languages.\n",
    "\n",
    "![Common memory](common_memory.png)\n",
    "\n",
    "## Some thoughts\n",
    "\n",
    "To load data from parquet into a high level language one needs to go from parquet to arrow to data structures in language X. Why not load directly from parquet to language X?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "timeperiod: string\n",
       "flow1: int32\n",
       "occupancy1: double\n",
       "speed1: double\n",
       "station: dictionary<values=int64, indices=int32>\n",
       "-- metadata --\n",
       "org.apache.spark.sql.parquet.row.metadata: {\"type\":\"struct\",\"fields\":[{\"name\":\"timeperiod\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow1\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy1\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed1\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow2\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy2\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed2\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow3\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy3\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed3\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow4\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy4\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed4\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow5\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy5\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed5\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow6\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy6\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed6\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow7\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy7\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed7\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"flow8\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"occupancy8\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}},{\"name\":\"speed8\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems_table = pems.read([\"timeperiod\", \"flow1\", \"occupancy1\", \"speed1\"])\n",
    "pems_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3932049, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metadata tells us things like the shape\n",
    "pems_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "786.4098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Approximate size of data fully in memory in MB\n",
    "pems_table.shape[0] * 25 * 8 / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.2M\t/Users/clark/data/pems/pems_sorted/station=402260\r\n",
      "7.0M\t/Users/clark/data/pems/pems_sorted/station=402261\r\n",
      "5.4M\t/Users/clark/data/pems/pems_sorted/station=402263\r\n",
      "8.2M\t/Users/clark/data/pems/pems_sorted/station=402264\r\n",
      "8.7M\t/Users/clark/data/pems/pems_sorted/station=402265\r\n",
      " 41M\t/Users/clark/data/pems/pems_sorted\r\n"
     ]
    }
   ],
   "source": [
    "# Compare to size on disk:\n",
    "! du -h ~/data/pems/pems_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Column at 0x1148952d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can pull out underlying pieces\n",
    "\n",
    "p1 = pems_table[1]\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataType(int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.ChunkedArray at 0x1148953f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.data.num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.Int32Array object at 0x1169602c8>\n",
       "[\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0\n",
       "]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.data.chunk(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.data.null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "5          0\n",
       "6          0\n",
       "7          0\n",
       "8          0\n",
       "9          0\n",
       "10         0\n",
       "11         0\n",
       "12         0\n",
       "13         0\n",
       "14         0\n",
       "15         0\n",
       "16         0\n",
       "17         0\n",
       "18         0\n",
       "19         0\n",
       "20         0\n",
       "21         0\n",
       "22         0\n",
       "23         0\n",
       "24         0\n",
       "25         0\n",
       "26         0\n",
       "27         0\n",
       "28         0\n",
       "29         0\n",
       "          ..\n",
       "3932019    0\n",
       "3932020    0\n",
       "3932021    0\n",
       "3932022    0\n",
       "3932023    0\n",
       "3932024    0\n",
       "3932025    0\n",
       "3932026    0\n",
       "3932027    0\n",
       "3932028    0\n",
       "3932029    0\n",
       "3932030    0\n",
       "3932031    0\n",
       "3932032    0\n",
       "3932033    0\n",
       "3932034    0\n",
       "3932035    0\n",
       "3932036    0\n",
       "3932037    0\n",
       "3932038    0\n",
       "3932039    0\n",
       "3932040    0\n",
       "3932041    0\n",
       "3932042    0\n",
       "3932043    0\n",
       "3932044    0\n",
       "3932045    0\n",
       "3932046    0\n",
       "3932047    0\n",
       "3932048    0\n",
       "Name: flow1, dtype: int32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pems_df = pems_table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timeperiod</th>\n",
       "      <th>flow1</th>\n",
       "      <th>occupancy1</th>\n",
       "      <th>speed1</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2016 00:00:05</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2016 00:00:35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2016 00:01:06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2016 00:01:35</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2016 00:02:05</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>402260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timeperiod  flow1  occupancy1  speed1 station\n",
       "0  01/01/2016 00:00:05      0         0.0     0.0  402260\n",
       "1  01/01/2016 00:00:35      0         0.0     0.0  402260\n",
       "2  01/01/2016 00:01:06      0         0.0     0.0  402260\n",
       "3  01/01/2016 00:01:35      0         0.0     0.0  402260\n",
       "4  01/01/2016 00:02:05      0         0.0     0.0  402260"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pems_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
