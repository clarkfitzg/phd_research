 Introduce myself
 Thanks for coming


============================================================


 The way going forward is through parallel software
 Showing Spark, but it could be any cluster
 I've worked on all these platforms
 Why make someone learn all these systems? Domain scientists don't care.


============================================================


 Conventional wisdom says that for performance you need to go to a lower
 level. Ie. from R, Rcpp, C, OpenCL, CUDA
 Norm's book on parallel programming: trading code readability for
 performance. (Originally Knuth?)
 Extreme example: Ethan approached to design circuits specific
 to a particular computation.


============================================================


 Sequential R code: just a script
 Data: text files, database, data in memory
 Platform: Laptop, cluster, GPU
 In contrast, compilation only examines code.


============================================================


 It stays high level, close to the problem domain of statistics / data
 analysis


============================================================


 I'm interested in it, and I'd like to see it through
 NOT the end goal of the project


============================================================


 Collected in 30 sec intervals
 Data Exceedingly noisy and dirty
 Available for public download


============================================================


 Fundamental diagram allows modeling change in congestion patterns
 and individual trip times
 It's statistics- a curve fitting problem
 Research traffic engineers experiment with many possible curves
 Note that observations are sparse in interesting areas of high density.
 Maybe 5% => sampling works poorly


============================================================


 Paper fit multiple lines minimizing the L1 norm of the residuals
 Can approximate this with robust linear models-
 standard errors get much better if use them for a year
 They used a handful of stations for 1 day
 Can do qualitatively different analysis with more data
 clustering, assessing policy effects


============================================================


 I appreciate R's succintness
 It's a fair amount of expert knowledge to make this run efficiently


============================================================


 Small perturbation of the code / analysis
 produce large changes in the required computation.
 It would be much better if a system could figure this out


============================================================


 Ideal case: Everything I'm doing requires moving the data from their
 servers, so it's necessarily inefficient.
 General principle: avoiding data movement
 As independent researchers we are less likely to deal with databases.


============================================================


 Intentionally simple, but we'll see some complexity
 This creates an intermediate vector of length n.
 Fails when n approaches limits of memory.


============================================================


 Assume that n can be split evenly
 so we can take the mean of the means.


============================================================


 Explain code
 The 2nd mean is a reduction, it can be generalized


============================================================


 Sending small things: one expression of code, then one number back from
 each worker. Keeps data local.


============================================================


 This uses SNOW, maybe most common way in R. Many other ways!
 Explain code
 Choose p in principled way
 substitute manipulates the expressions
 then we evaluate it on the cluster


============================================================




============================================================


 The point is that the system should create the parallel code for us


============================================================


 Looking at the first part of the function, it calls right into C
 So we can't do it directly
 Options in order of ambition
 `indicate how vectorized' - meaning it's vectorized in multiple args


============================================================


 Thanks to Duncan, Gabe Becker, Deb Nolan, Roger Peng
 Gabe Becker put on CRAN this week, worked on during his PhD
 Following motivated by DSI use case


============================================================


 Tells us when variables are used => need to be available on workers
 Can run the first two threads in parallel


============================================================


 This has to be answered for every piece of code, every time
 Explain each fig
 If there wasn't any overhead, parallel would always be better choice
 If you start up a process, you want to keep it around


============================================================


 This is one reason that it's more appealing to do this in R
 These are all essentially variants of `Map' in mapreduce


============================================================


 These are excellent tools
 But the problem is that one is tied to particular package / architecture


============================================================


 Point of this project is not to build another tool-
 Rather to use analysis of the code itself as the mechanism.
 I want to reuse as much infrastructure as possible


============================================================


 Dimensions, types allow preallocation
 Randomization lets us do software alchemy
 Index in a database allows us to select some groups faster


============================================================


 File seeking takes microseconds


============================================================


 Given sequential code, some data source, and a platform we try to create
 reasonably efficient parallel code.


============================================================


 I like my data dirty
 The computation graphs target different platforms, mostly numeric data
 An issue with larger systems is that they require you to express your
 computation in their own particular regime.


============================================================


 Starting with this because it's feasible.
 Certainly plan to go beyond.


============================================================


 As I mentioned above, this is where data actually lives, so lets think
 hard about how to move the computation there.


============================================================


 CalTrans uses this to study congestion patterns and inform policy


============================================================


 Parameters- things that can be controlled


============================================================


