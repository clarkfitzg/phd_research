## Reflections and Lessons Learned

Wed Nov 29 07:10:33 PST 2017

ddR is on my radar again, since they have another intern working on it, and
people have been asking. Now that some time has passed and I'm a little
more experienced with computing on larger data sets I wanted to reflect on
ddR, ask some hard questions, and offer some suggestions on how to improve
it. The following is my opinion, take it for what it's worth.

### Social

__Ownership__ Who will maintain ddR and push it forward? Officially the
repo is on HP Vertica's Github site, but the contributors have since moved
to other companies. So further efforts may require forking the project.
For ddR to survive it needs an active owner. People will only be interested
in owning it if they can use it to solve their own problems.

__Motivating Use Cases__ Where will ddR be used in the wild? Which
users will benefit from this? I believe the intended user group is people
developing distributed algorithms in R.

### Technical

From ddR's README:

> The 'ddR' package aims to provide an unified R interface for writing
> parallel and distributed applications.

Bryan Lewis said something along the lines of: "Maybe R itself is
expressive enough." He meant that R's class system suffices as an
abstraction layer. I tend to agree.
[gpuR](https://github.com/cdeterman/gpuR) is a nice example of this.

__Common Implementation__ Is it a good idea to share the implementation of
algorithms across different parallel / distributed systems? SNOW, Unix
multicore, GPU's, slurm, MPI, tensorflow, and Hadoop offer quite different
computational models at different levels of abstraction. They may also
offer hybrid approaches. For example, the [rslurm
package](https://cran.r-project.org/web/packages/rslurm/index.html) uses
slurm and multicore. If an algorithm is general enough to run on all of
these systems then this means the algorithm can only rely on features
common to all. Then we miss out on performance benefits offered by more
specialized systems.

The DBI package provides a standard interface from R to relational
databases, so it's somewhat like ddR. DBI differs from ddR in that it has a
much more well defined task. A relational database interface essentially
manages connections, sends queries, and returns rows. DBI doesn't need to
support the much broader and more open ended task of general purpose
computation.  Many languages besides R defined standard database interfaces
like DBI. But I'm not aware of anything like ddR in other languages; if
they exist we should definitely be learning from them.

__Data__ The overhead associated with loading, moving, and saving data can
take up the majority of the total run time. This is especially true with
distributed data, and it's why we have systems like Hadoop. ddR doesn't
help much with data management, so it misses the chance to solve a real
pain point.

There's also the related question of how to use R to compute on data stored
in some native format. I don't think there are simple general answers to this
question.

__Abstraction__ It's not clear that the abstraction layer in ddR reduces
complexity for the users. I found that I needed to thoroughly understand
both ddR and the underlying distributed system in order to use them.

__Complexity__ The code base is complex for what it does. I did what
I could to simplify it, but there's more to be done.

### Suggestions

Restrict ddR to only providing `lapply` on a distributed list. Lists and
`lapply` are simple and powerful. Let ddR be proud that it uses the lowest
common denominator of functionality. SNOW's success shows that this is
still very useful. Focusing and narrowing down the scope will produce a
more manageable codebase for ddR.

Do a disk based backend for large data sets. This would be a distributed
list where each chunk of the lists is an .Rdata file stored on disk. The
bigmemory and ff packages do similar things in different ways. Maybe it's
like we're going back to the S language, but I think there's a place in the
R ecosystem for this.

Identify a concrete example use case on public data where ddR adds value,
ideally by doing something beyond what current packages provide.



## Personal

Fri Oct  7 11:41:18 PDT 2016

The project has been done for a few weeks, having a final call this
afternoon.

Now I am not convinced that `ddR`'s model is necessary. Distributed systems
are so different, this seems excessively ambitious to work well and
perform.

Sign a contract and get paid first. Verbal agreements insufficient. Pay
hassles every single time and late payments were not fun.

I was under the impression that this was much more
mature than it is. 

Who owns the project? If it's a company, are they motivated to maintain it? If it's
an individual, are they likely to stick it out?

Use the software before starting work on it. Even a very simple application
to real data analysis revealed serious memory and speed problems with the
base implementation. 

Be careful with low activity. Although there was buzz in the forms of blogs
and Github watchers, I'm not aware of applications of the
software on anything other than simulated data. And there was no activity
on the Github account in 8 months or so.

Choose dependencies wisely. I thought I was doing this when by keeping an
open dialogue with RStudio, but they pulled the rug out from us by
incorporating `sparkapi` into `sparklyr`. In my mind they should still be
separate packages.

Check for code quality. This is important for public projects. Is one
naming scheme CamelCase or under_score followed? Are functions easy to read
and composed from nice modular units? How much code is repeated? Are there
functions or classes with no apparent purpose? Is the user facing API
clearly defined and minimal?
