## Reflections and Lessons Learned

Wed Nov 29 07:10:33 PST 2017

ddR is on my radar again, since they have another intern working on it, and
people have been asking.

### Social

__Ownership__ Who will maintain ddR and push it forward? Officially the
repo is on Vertica's Github site, but the contributors have since left
Vertica. So further efforts are probably going to require forking the
project.

__Motivating Use Cases__ Where will ddR be used in the wild? Which
users will benefit from this? I believe the intended user group is people
developing distributed algorithms in R.

### Technical

Bryan Lewis said something along the lines of: "Maybe R itself is
expressive enough." He meant that R's class system suffices as an
abstraction layer. [gpuR](https://github.com/cdeterman/gpuR) is a nice
example of this.

__Common Implementation__ Is it a good idea to share the implementation of
algorithms across different parallel / distributed systems? SNOW, Unix
fork, GPU's, slurm, MPI, tensorflow, and Hadoop offer quite different
computational models at different levels of abstraction. They may also
offer hybrid approaches. The [rslurm
package](https://cran.r-project.org/web/packages/rslurm/index.html) uses
slurm and a Unix fork through `parallel::mclapply`. If an algorithm is
general enough to run on all of them then this means the algorithm can only
rely on features common to all. Then we miss out on performance benefits
offered by more specialized systems.

In 2016 Spark didn't have efficient machine learning algorithms even in their own system, so
it's unlikely that adding an abstraction layer will help performance.

Contrast this with DBI, a similar project.


__Data__ The overhead associated with loading, moving, and saving data can
take up the majority of the total run time. This is especially true with
distributed data, and it's why we have systems like Hadoop. ddR doesn't
help much with data management, so it misses the chance to solve a real
pain point.

There's also the related question of how to use R to compute on data stored
in a native system like Spark.

__Abstraction__ It's not clear that the abstraction layer in ddR reduces
complexity. I found that I needed to thoroughly understand both ddR and
the underlying distributed system in order to use them.

__Complexity__ The code base was quite complex for what it does.

### Suggestions

Identify a concrete example use case on public data where this will add value.

Just have `lapply` on a distributed list. 
Celebrate that we have to use the lowest common denominator.
This is the lowest common
denominator. It has a direct analog in base R. Code shouldn't be
excessively complex. It offers users something different beyond data
frames- ie. partools already does distributed data frames.

Offer a memory and disk based system.

## Personal

Fri Oct  7 11:41:18 PDT 2016

The project has been done for a few weeks, having a final call this
afternoon.

Now I am not convinced that `ddR`'s model is necessary. Distributed systems
are so different, this seems excessively ambitious to work well and
perform.

Sign a contract and get paid first. Verbal agreements insufficient. Pay
hassles every single time and late payments were not fun.

I was under the impression that this was much more
mature than it is. 

Who owns the project? If it's a company, are they motivated to maintain it? If it's
an individual, are they likely to stick it out?

Use the software before starting work on it. Even a very simple application
to real data analysis revealed serious memory and speed problems with the
base implementation. 

Be careful with low activity. Although there was buzz in the forms of blogs
and Github watchers, I'm not aware of applications of the
software on anything other than simulated data. And there was no activity
on the Github account in 8 months or so.

Choose dependencies wisely. I thought I was doing this when by keeping an
open dialogue with RStudio, but they pulled the rug out from us by
incorporating `sparkapi` into `sparklyr`. In my mind they should still be
separate packages.

Check for code quality. This is important for public projects. Is one
naming scheme CamelCase or under_score followed? Are functions easy to read
and composed from nice modular units? How much code is repeated? Are there
functions or classes with no apparent purpose? Is the user facing API
clearly defined and minimal?
