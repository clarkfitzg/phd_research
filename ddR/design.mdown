# Design Document

# Spark 

We need to execute user defined functions on local dataframes, matrices, and lists.
Dataframes and matrices can be partitioned by both rows and columns. The
underlying idea of Spark dataframes is RDD's consisting of rows, which is
not compatible with partitioning the columns.

In order to use Spark in this more general way the idea is to have it
simply act as a key value pair which stores serialized binary R objects and
can run R code:

row |   column  |   BLOB 
----|-----------|-------
1   |   1       |   XXX
1   |   2       |   XXX
... |   ...     |   ...

Here (row, column) is the key and BLOB is an RDS object.

__Pros__

- This is a general design that would work for many systems, and
  possibly even the serialized version

__Cons__

- Not using Spark's ability to store data in anything other than a binary
  format.

## Dependencies

There are two options for packages that could be depended on to
implement Spark as a backend.

### sparkapi

With this lower level approach we can store the distributed object directly
in a [Pair
RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaPairRDD)

__Pros__

- Pair RDD seems like a natural container for this
- Low level access into any general methods we may need for repartitioning,
  etc.

__Cons__

- Requires bringing in and maintaining a minimal set of private SparkR
  code, like Javier's example
- Requires more knowledge of Spark / Scala / Java
- Probably will take longer to develop

### SparkR

This approach consists of storing the data in a table as above, and then
using SparkR's
[dapply](http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/sparkr.html#applying-user-defined-function)
to launch the R processes on the workers.

__Pros__

- Building on DataFrames provides compatibility that may make many things
  easier, such as loading and saving data.
- Relying more on Apache Spark maintained code

__Cons__

- More difficult to install since SparkR is not on CRAN
- Long term stability and support of necessary code in SparkR is not
  guaranteed
- May be more overhead from running through SparkR 




# Methods

A ddR backend should implement the following methods:

## object.R

__initialize__ Function `new` calls this to create a new parallel object

__combine__ 

```
a = dlist(1:5, 2)
b = dlist(1:10)

combine(ddR:::ddR.env$driver, list(a, b))

Error in if (!is.dlist(dobj)) ans <- t(ans) :
  missing value where TRUE/FALSE needed
```

__get_parts__ 

Only used in one place:

```
~/dev/ddR/ddR/R $ grep "get_parts(" *
dobject.R:  partitions <- get_parts(dobj, index)
```

TODO: understand `combine` and `getparts`. I'm not having any luck using
these with default parallel backend.

__do_collect__ Move from storage to local list, matrix, or data.frame in
calling context.

## driver.R

__init__ Called when the backend driver is initialized.

__shutdown__ Called when the backend driver is shutdown.

__do_dmapply__ Backend-specific dmapply logic.



